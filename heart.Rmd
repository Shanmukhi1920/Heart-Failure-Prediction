---
title: "Heart Failure Prediction data analysis"
author: "Shanmukhi"
date: "2022-11-09"
output:
  pdf_document: default
  html_document: default
---

## Loading the required libararies

```{r}
library(tidyverse)
library(dplyr)
library(caret)
library(e1071)
library(ggplot2)
library(rpart)
library(ggfortify)
library(kknn)
```

## setting the working directory

```{r}
setwd("D:/DSC441/HW5")
getwd()
```
# **a.Data Gathering and Integration**

## Took Heart Failure Prediction Dataset from Kaggle
```{r}
# loading the dataset
heart<-read.csv("heart.csv",header=TRUE,sep=",")
# Top 6 columns
head(heart)
```
```{r}
str(heart)
```

```{r}
# Columns in the dataset
names(heart)
```
# **b.Data Exploration**
```{r}
summary(heart)
```
```{r}
# Grouping and Summarizing
# Grouping by Chest Pain Type
heart %>% group_by(heart$ChestPainType) %>% summarise("count"=n())
# Grouping by Heart diseases
heart %>% group_by(heart$HeartDisease) %>% summarise("count"=n())
# Grouping by Resting ECG
heart %>% group_by(heart$RestingECG) %>% summarise("count"=n())
#Grouping by ST_Slope
heart %>% group_by(heart$ST_Slope) %>% summarise("count"=n())
#Groupby sex and find mean of Cholesterol
heart %>% group_by(heart$Sex) %>% summarize(avg_cholesterol = mean(Cholesterol,na.rm=TRUE))
#Groupby Age and find mean of Max heart rate
heart %>% group_by(heart$Sex) %>% summarize(avg_heartrate = mean(MaxHR,na.rm=TRUE))
# Group by Sex find the total of heart disease people
heart %>% group_by(Sex) %>% select(Sex, HeartDisease) %>% table() %>% head()
# Group by Chest Pain type find the total of heart disease people
heart %>% group_by(ChestPainType) %>% select(ChestPainType, HeartDisease) %>% table() %>% head()
```
## Data distributions
```{r}
# Histogram's of numerical variables
ggplot(heart, aes(Age)) + geom_histogram(binwidth = 7)
ggplot(heart, aes(MaxHR)) + geom_histogram(binwidth = 15)
ggplot(heart, aes(RestingBP)) + geom_histogram(binwidth = 20)
ggplot(heart, aes(Cholesterol)) + geom_histogram(binwidth = 50)
ggplot(heart, aes(Oldpeak)) + geom_histogram(binwidth = 2)
ggplot(heart, aes(FastingBS)) + geom_histogram(binwidth = 0.5)
```
```{r}
# Bar plots for categorical 
ggplot(heart, aes(x=Sex)) + geom_bar()
ggplot(heart, aes(x=ChestPainType)) + geom_bar()
ggplot(heart, aes(x=RestingECG)) + geom_bar()
ggplot(heart, aes(x=ExerciseAngina)) + geom_bar()
ggplot(heart, aes(x=ST_Slope)) + geom_bar()
```
## Relationship between variables
```{r}
# Bar plot of Chest Pain type stacked by Sex
Cpt <- ggplot(heart, aes(x=ChestPainType, fill=Sex))
Cpt + geom_bar(position="stack")
```
```{r}
# Scatter Plot of Age vs Resting BP
ggplot(heart, aes(Age, RestingBP)) + geom_point()
```
```{r}
# Box plot of ECG vs Old peak
ggplot(heart, aes(x=RestingECG,y=Oldpeak))+geom_boxplot()
```
```{r}
# Scatter plot of Cholesterol vs Heart rate
ggplot(heart, aes(Cholesterol,MaxHR)) + geom_point()
```
```{r}
# Box plot of Heart Diseases vs Age
ggplot(heart, aes(x=HeartDisease,y=Age,group=HeartDisease))+geom_boxplot()
```

# **c.Data Cleaning**
```{r}
#checking for null values
colSums(is.na(heart))
```
## Implies that there are no null values
```{r}
# Looking at numerical columns distribution by box plots to look at outliers
boxplot(heart$RestingBP,xlab="RestingBP")
boxplot(heart$Cholesterol,xlab="Cholesterol")
boxplot(heart$MaxHR,xlab="Max Heart Rate")
boxplot(heart$Oldpeak,xlab="OldPeak")
boxplot(heart$Age,xlab="Age")

```
```{r}
# Removing the values for Numeric columns which are in range quartiles +/- 1.5 * IQR
Q1 <- quantile(heart$RestingBP, .25)
Q3 <- quantile(heart$RestingBP, .75)
IQR <- IQR(heart$RestingBP)
heart<- subset(heart, heart$RestingBP> (Q1 - 1.5*IQR) & heart$RestingBP< (Q3 + 1.5*IQR))

Q1 <- quantile(heart$Cholesterol, .25)
Q3 <- quantile(heart$Cholesterol, .75)
IQR <- IQR(heart$Cholesterol)
heart<- subset(heart, heart$Cholesterol> (Q1 - 1.5*IQR) & heart$Cholesterol< (Q3 + 1.5*IQR))

Q1 <- quantile(heart$MaxHR, .25)
Q3 <- quantile(heart$MaxHR, .75)
IQR <- IQR(heart$MaxHR)
heart<- subset(heart, heart$MaxHR> (Q1 - 1.5*IQR) & heart$MaxHR< (Q3 + 1.5*IQR))

Q1 <- quantile(heart$Oldpeak, .25)
Q3 <- quantile(heart$Oldpeak, .75)
IQR <- IQR(heart$Oldpeak)
heart<- subset(heart, heart$Oldpeak> (Q1 - 1.5*IQR) & heart$Oldpeak< (Q3 + 1.5*IQR))

Q1 <- quantile(heart$Age, .25)
Q3 <- quantile(heart$Age, .75)
IQR <- IQR(heart$Age)
heart<- subset(heart, heart$Age> (Q1 - 1.5*IQR) & heart$Age< (Q3 + 1.5*IQR))

dim(heart)
```
```{r}
# converting ExerciseAngina column from charater to integer(0,1)
heart$ExerciseAngina<-ifelse(heart$ExerciseAngina=="Y",1,0)
head(heart)
```
```{r}
# summary of the dataset
summary(heart)
```

# **d.Data Pre processing**
```{r}
# creating bins for cholesterol
heart <- heart %>%mutate(ChRange = cut(Cholesterol, breaks = c(-1,150,200,500),labels=c("Normal","BorderlineHigh","High")))
head(heart)
```
```{r}
# Normalizing the data using Standardization
heart_norm<- heart %>% select(-c(HeartDisease))
preprocess <- preProcess(heart_norm, method=c("center","scale"))
norm <- predict(preprocess, heart_norm)
norm$HeartDisease<-heart$HeartDisease
#head of normalized data
head(norm)
summary(norm)
```

```{r}
# creating dummy variables
norm_dummy <- dummyVars(~ ., data = norm)
norm_dummies <- as.data.frame(predict(norm_dummy, newdata = norm))
head(norm_dummies)
```

```{r}
heart_dummies <- as.data.frame(norm_dummies)
heart_dummies <- na.omit(heart_dummies)
head(heart_dummies)
```
# **e.Clustering**
```{r}
library(factoextra)
library(cluster)
```

```{r}
set.seed(13)
heart2<-heart_dummies %>% select(-c(HeartDisease))
preprocess<-preProcess(heart2,method=c("center","scale"))
pred<-predict(preprocess,heart2)
# finding optimal number of clusters
fviz_nbclust(heart2, kmeans, method = "wss")
fviz_nbclust(heart2, kmeans, method = "silhouette")
gap_stat <- clusGap(heart2,
                    FUN = kmeans,
                    nstart = 25,
                    K.max = 10,
                    B = 50)
# plot number of clusters vs. gap statistic
fviz_gap_stat(gap_stat)
```
## choosen K=3 as they were the optimal number of clusters

```{r}
# Using K means with 3 centers
km <- kmeans(heart2, centers = 3, nstart = 25)
km
```
```{r}
# Visualizaing the Clusters
fviz_cluster(km, data = heart2)
```
```{r}
# finding means of variables in each cluster
aggregate(heart2, by=list(cluster=km$cluster), mean)
```

```{r}
# Adding clusters to dataset
heart2_cluster<- cbind(heart2, cluster = km$cluster)
head(heart2_cluster)
```
```{r}
# Calculating PCA Projectionand coloring by clusters
# Calculate PCA
pca = prcomp(pred)
# Save as dataframe
rotated_data = as.data.frame(pca$x)
# Add original labels as a reference
rotated_data$Clusters = as.factor(km$cluster)
# Plot and color by 
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Clusters)) + geom_point()
```

# **f.Classification**
```{r}
# 1.SVM - Using Grid Search for choosing best parameters
train_control = trainControl(method = "cv", number = 10)
# tuning C value
grid <- expand.grid(C = 10^seq(-3,3,0.5))
# Creating train/test partition
index = createDataPartition(y=heart_dummies$HeartDisease, p=0.8, list=FALSE)
train_set = heart_dummies[index,]
test_set = heart_dummies[-index,]
# Using svm classifier
svm_grid <- train(as.factor(HeartDisease)~., data = train_set, method = "svmLinear", 
                  trControl = train_control, tuneGrid = grid)
svm_grid
```
```{r}
#2. K Nearest Neighbor - using kknn library as it gives us option to tune grid
# Tuning the K max,Kernel and distance parameters 
tuneGrid <- expand.grid(kmax = 3:10,                        
                        kernel = c("rectangular", "cos"),  
                        distance = 1:3)                    
# Using Knn
kknn_fit <- train(as.factor(HeartDisease) ~ ., 
                  data = train_set,
                  method = 'kknn',
                  trControl = train_control,
                  tuneGrid = tuneGrid)
kknn_fit
```
## SVM has accuarcy of 85.15% on training set with C=0.001 and kknn has 86.6% with kmax=8 and p set to 1 and kernel as rectangular.

# **g.Evaluation**
```{r}
# choosing best Classifier by prediction and by plotting confusion matrix
# Prediction using test set
pred_test<-predict(svm_grid,test_set)
pred_test1<-predict(kknn_fit,test_set)
#Confusion Matrix
cm<-confusionMatrix(as.factor(test_set$HeartDisease), pred_test)
cm
cm1<-confusionMatrix(as.factor(test_set$HeartDisease), pred_test1)
cm1
```
## SVM has accuracy of 85.5% on test set which is higher than Knn which has accuracy of 81.8% on test set.

## Even though both the algorithmns equally predict the False Negatives, SVM has ony 4 False positive values which made to choose SVM as best classifier

```{r}
# Prediction using test set
pred_final<-predict(svm_grid,test_set)
# Confusion Matrix
cm_final<-confusionMatrix(as.factor(test_set$HeartDisease), pred_final)
cm_final
```
```{r}
# calculation precision and recall
cm_final$byClass["Precision"]
cm_final$byClass["Recall"]
```
```{r}
# ROC curve
library(pROC)
roc_svm<-roc(response=test_set$HeartDisease,predictor=as.numeric(pred_final))
plot(roc_svm, print.auc=TRUE)
```

Classifier's True Positives are 57, False Positives are 4, True Negatives are 61 and False Negatives are 16.

Specially in this case False Positives are important as they lead to unnecessary treatment and False negatives can lead to false diagnosis which is serious as disease is ignored.

Here the classifier has higher value of recall which means algorithm returns most of relevant results.

Area under ROC being 86% signifies that the binary classifier has better model performance while distinguishing.



